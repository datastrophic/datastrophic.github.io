<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>spark on datastrophic</title>
    <link>https://datastrophic.github.io/tags/spark/</link>
    <description>Recent content in spark on datastrophic</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Oct 2017 00:00:00 +0000</lastBuildDate><atom:link href="https://datastrophic.github.io/tags/spark/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Spark JobServer: from Spark Standalone to Mesos, Marathon and Docker</title>
      <link>https://datastrophic.github.io/spark-jobserver-from-spark-standalone-to-mesos-marathon-and-docker-part-i/</link>
      <pubDate>Thu, 12 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://datastrophic.github.io/spark-jobserver-from-spark-standalone-to-mesos-marathon-and-docker-part-i/</guid>
      <description>After several years of running Spark JobServer workloads, the need for better availability and multi-tenancy emerged across several projects author was involved in. This blog post covers design decisions made to provide higher availability and fault tolerance of JobServer installations, multi-tenancy for Spark workloads, scalability and failure recovery automation, and software choices made in order to reach these goals.
Spark JobServer Spark JobServer is widely used across a variety of reporting and aggregating systems.</description>
    </item>
    
    <item>
      <title>Apache Spark: core concepts, architecture and internals</title>
      <link>https://datastrophic.github.io/core-concepts-architecture-and-internals-of-apache-spark/</link>
      <pubDate>Thu, 03 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>https://datastrophic.github.io/core-concepts-architecture-and-internals-of-apache-spark/</guid>
      <description>This post covers core concepts of Apache Spark such as RDD, DAG, execution workflow, forming stages of tasks, and shuffle implementation and also describes the architecture and main components of Spark Driver. There&amp;rsquo;s a github.com/datastrophic/spark-workshop project created alongside this post which contains Spark Applications examples and dockerized Hadoop environment to play with. Slides are also available at slideshare.
Intro Spark is a generalized framework for distributed data processing providing functional API for manipulating data at scale, in-memory data caching, and reuse across computations.</description>
    </item>
    
  </channel>
</rss>
