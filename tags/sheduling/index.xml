<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>sheduling on datastrophic</title>
    <link>https://datastrophic.github.io/tags/sheduling/</link>
    <description>Recent content in sheduling on datastrophic</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 27 Mar 2016 00:00:00 +0000</lastBuildDate><atom:link href="https://datastrophic.github.io/tags/sheduling/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Resource Allocation in Mesos: Dominant Resource Fairness</title>
      <link>https://datastrophic.github.io/resource-allocation-in-mesos-dominant-resource-fairness-explained/</link>
      <pubDate>Sun, 27 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>https://datastrophic.github.io/resource-allocation-in-mesos-dominant-resource-fairness-explained/</guid>
      <description>Apache Mesos provides a unique approach to cluster resource management called two-level scheduling: instead of storing information about available cluster resources in a centralized manner it operates with a notion of resource offers which slave nodes advertise to running frameworks via Mesos master, thus keeping the whole system architecture concise and scalable. Master&amp;rsquo;s allocation module is responsible for making the decisions about which application should receive the next resource offer and it relies on Dominant Resource Fairness(DRF) algorithm for making these decisions.</description>
    </item>
    
    <item>
      <title>Apache Spark: core concepts, architecture and internals</title>
      <link>https://datastrophic.github.io/core-concepts-architecture-and-internals-of-apache-spark/</link>
      <pubDate>Thu, 03 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>https://datastrophic.github.io/core-concepts-architecture-and-internals-of-apache-spark/</guid>
      <description>This post covers core concepts of Apache Spark such as RDD, DAG, execution workflow, forming stages of tasks, and shuffle implementation and also describes the architecture and main components of Spark Driver. There&amp;rsquo;s a github.com/datastrophic/spark-workshop project created alongside this post which contains Spark Applications examples and dockerized Hadoop environment to play with. Slides are also available at slideshare.
Intro Spark is a generalized framework for distributed data processing providing functional API for manipulating data at scale, in-memory data caching, and reuse across computations.</description>
    </item>
    
  </channel>
</rss>
