<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on datastrophic</title>
    <link>https://datastrophic.github.io/post/</link>
    <description>Recent content in Posts on datastrophic</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 04 Oct 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://datastrophic.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Kubeflow Training Operators and Istio: solving the proxy sidecar lifecycle problem for AI/ML workloads</title>
      <link>https://datastrophic.github.io/kubeflow-training-operators-and-istio-solving-the-proxy-sidecar-lifecycle-problem-for-aiml-workloads/</link>
      <pubDate>Mon, 04 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://datastrophic.github.io/kubeflow-training-operators-and-istio-solving-the-proxy-sidecar-lifecycle-problem-for-aiml-workloads/</guid>
      <description>With Kubeflow gaining traction in the community and its early adoption in enterprises, security and observability concerns become more and more important. Many organizations that are running AI/ML workloads, operate with sensitive personal or financial data and have stricter requirements for data encryption, traceability, and access control. Quite often, we can see the use of the Istio service mesh for solving these problems and gaining other benefits of the rich functionality it provides.</description>
    </item>
    
    <item>
      <title>Spark JobServer: from Spark Standalone to Mesos, Marathon and Docker</title>
      <link>https://datastrophic.github.io/spark-jobserver-from-spark-standalone-to-mesos-marathon-and-docker-part-i/</link>
      <pubDate>Thu, 12 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://datastrophic.github.io/spark-jobserver-from-spark-standalone-to-mesos-marathon-and-docker-part-i/</guid>
      <description>After several years of running Spark JobServer workloads, the need for better availability and multi-tenancy emerged across several projects author was involved in. This blog post covers design decisions made to provide higher availability and fault tolerance of JobServer installations, multi-tenancy for Spark workloads, scalability and failure recovery automation, and software choices made in order to reach these goals.
Spark JobServer Spark JobServer is widely used across a variety of reporting and aggregating systems.</description>
    </item>
    
    <item>
      <title>Resource Allocation in Mesos: Dominant Resource Fairness</title>
      <link>https://datastrophic.github.io/resource-allocation-in-mesos-dominant-resource-fairness-explained/</link>
      <pubDate>Sun, 27 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>https://datastrophic.github.io/resource-allocation-in-mesos-dominant-resource-fairness-explained/</guid>
      <description>Apache Mesos provides a unique approach to cluster resource management called two-level scheduling: instead of storing information about available cluster resources in a centralized manner it operates with a notion of resource offers which slave nodes advertise to running frameworks via Mesos master, thus keeping the whole system architecture concise and scalable. Master&amp;rsquo;s allocation module is responsible for making the decisions about which application should receive the next resource offer and it relies on Dominant Resource Fairness(DRF) algorithm for making these decisions.</description>
    </item>
    
    <item>
      <title>Apache Spark: core concepts, architecture and internals</title>
      <link>https://datastrophic.github.io/core-concepts-architecture-and-internals-of-apache-spark/</link>
      <pubDate>Thu, 03 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>https://datastrophic.github.io/core-concepts-architecture-and-internals-of-apache-spark/</guid>
      <description>This post covers core concepts of Apache Spark such as RDD, DAG, execution workflow, forming stages of tasks, and shuffle implementation and also describes the architecture and main components of Spark Driver. There&amp;rsquo;s a github.com/datastrophic/spark-workshop project created alongside this post which contains Spark Applications examples and dockerized Hadoop environment to play with. Slides are also available at slideshare.
Intro Spark is a generalized framework for distributed data processing providing functional API for manipulating data at scale, in-memory data caching, and reuse across computations.</description>
    </item>
    
    <item>
      <title>Data processing platforms architectures with SMACK: Spark, Mesos, Akka, Cassandra and Kafka</title>
      <link>https://datastrophic.github.io/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka/</link>
      <pubDate>Wed, 16 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>https://datastrophic.github.io/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka/</guid>
      <description>This post is a follow-up of the talk given at Big Data AW meetup in Stockholm and focused on different use cases and design approaches for building scalable data processing platforms with SMACK(Spark, Mesos, Akka, Cassandra, Kafka) stack. While stack is really concise and consists of only several components it is possible to implement different system designs which list not only purely batch or stream processing, but more complex Lambda and Kappa architectures as well.</description>
    </item>
    
    <item>
      <title>Cassandra 2.1 Counters: Testing Consistency During Node Failures</title>
      <link>https://datastrophic.github.io/evaluating-cassandra-2-1-counters-consistency/</link>
      <pubDate>Thu, 03 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>https://datastrophic.github.io/evaluating-cassandra-2-1-counters-consistency/</guid>
      <description>For some cases such as the ones present in AdServing, the counters come really handy to accumulate totals for events coming into a system compared to batch aggregates. While distributed counters consistency is a well-known problem Cassandra counters in version 2.1 are claimed to be more accurate compared to the prior ones. This post describes the approach and the results of Cassandra counters consistency testing in different failure scenarios such as rolling restarts, abnormal termination of nodes, and network splits.</description>
    </item>
    
  </channel>
</rss>
