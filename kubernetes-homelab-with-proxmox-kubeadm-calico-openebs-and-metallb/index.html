<!doctype html><html lang=en-us>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1">
<meta property="og:site_name" content="datastrophic">
<meta property="og:type" content="article">
<meta property="og:image" content="https://datastrophic.io//img/datastrophic-strip-1.png">
<meta property="twitter:image" content="https://datastrophic.io//img/datastrophic-strip-1.png">
<meta name=title content="The Ultimate Kubernetes Homelab Guide: From Zero to Production Cluster On-Premises">
<meta property="og:title" content="The Ultimate Kubernetes Homelab Guide: From Zero to Production Cluster On-Premises">
<meta property="twitter:title" content="The Ultimate Kubernetes Homelab Guide: From Zero to Production Cluster On-Premises">
<meta name=description content="A blog about distributed systems, data platforms, and AI infrastructure">
<meta property="og:description" content="A blog about distributed systems, data platforms, and AI infrastructure">
<meta property="twitter:description" content="A blog about distributed systems, data platforms, and AI infrastructure">
<meta property="twitter:card" content="summary">
<meta name=keyword content="Spark, Mesos, Kubernetes, Kubeflow, Istio, SMACK stack, Cassandra, Kafka">
<link rel="shortcut icon" href=/img/favicon.ico>
<title>The Ultimate Kubernetes Homelab Guide: From Zero to Production Cluster On-Premises</title>
<link rel=canonical href=/kubernetes-homelab-with-proxmox-kubeadm-calico-openebs-and-metallb/>
<link rel=stylesheet href=/css/bootstrap.min.css>
<link rel=stylesheet href=/css/hugo-theme-cleanwhite.min.css>
<link rel=stylesheet href=/css/zanshang.css>
<link href=//cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css rel=stylesheet type=text/css>
<link rel=stylesheet href=https://datastrophic.io/css/footer.css><link rel=stylesheet href=https://datastrophic.io/css/pager.css><link rel=stylesheet href=https://datastrophic.io/css/overrides.css>
<script src=/js/jquery.min.js></script>
<script src=/js/bootstrap.min.js></script>
<script src=/js/hux-blog.min.js></script>
</head>
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
<div class=container-fluid>
<div class="navbar-header page-scroll">
<button type=button class=navbar-toggle>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span>
</button>
</div>
<div id=huxblog_navbar>
<div class=navbar-collapse>
<ul class="nav navbar-nav navbar-right">
<li>
<a href=/>Home</a>
</li>
<li><a href=/top/archive/>ARCHIVE</a></li>
<li><a href=/top/about/>ABOUT</a></li>
</ul>
</div>
</div>
</div>
</nav>
<script>var $body=document.body,$toggle=document.querySelector('.navbar-toggle'),$navbar=document.querySelector('#huxblog_navbar'),$collapse=document.querySelector('.navbar-collapse');$toggle.addEventListener('click',handleMagic);function handleMagic(a){$navbar.className.indexOf('in')>0?($navbar.className=" ",setTimeout(function(){$navbar.className.indexOf('in')<0&&($collapse.style.height="0px")},400)):($collapse.style.height="auto",$navbar.className+=" in")}</script>
<style type=text/css>header.intro-header{background-image:url(/img/datastrophic-strip-1.png);color:#3a4145}</style>
<header class=intro-header style=background-image:url(/img/datastrophic-strip-1.png)>
<div class=container>
<div class=row>
<div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
<div class=post-heading>
<div class=tags>
<a class=tag href=/tags/kubernetes title=kubernetes>
kubernetes
</a>
<a class=tag href=/tags/devops title=devops>
devops
</a>
</div>
<h1>The Ultimate Kubernetes Homelab Guide: From Zero to Production Cluster On-Premises</h1>
<span class=meta>
Posted on
December 1, 2021
</span>
</div>
</div>
</div>
</div>
</header>
<article>
<div class=container>
<div class=row>
<div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
post-container">
<p>Whether you&rsquo;re looking for a more powerful development environment or a production-grade Kubernetes cluster for experiments, this guide provides end-to-end deployment and configuration instructions to get the cluster up and running.</p>
<p>The first part of this guide covers the planning and provisioning of the infrastructure with Proxmox and Terraform. The second part is dedicated to installing Kubernetes and essential software such as Calico for networking, OpenEBS for volume provisioning, and MetalLB for network load balancing. At the end, the guide provides steps for deploying the Kubernetes
Dashboard with restricted permissions.</p>
<h2 id=planning-and-provisioning-the-infrastructure>Planning and provisioning the infrastructure</h2>
<p>This section contains basic information on how to get a virtual infrastructure up and running in an automated manner. If you already have the infrastructure ready (whether it&rsquo;s a multi-server rack or several pre-provisioned VMs) - just skip ahead to the <a href=#installing-the-kubernetes-cluster-and-essentials>Kubernetes deployment</a> part of this guide. However, if you just have a spare server or a commodity workstation you&rsquo;d like to use, then this section might be helpful for bootstrapping the infrastructure from scratch.</p>
<h3 id=deployment-layout>Deployment layout</h3>
<p>There are several options for how the target Kubernetes cluster will fit into the existing network and how clients will access it. Also, the target cluster may consist of several hardware nodes, a virtualized environment, or a hybrid of both. Let&rsquo;s look at the following layout:</p>
<p>
<img src=/blog/2021-12-01/infrastructure-layout.png alt="Infrastructure Layout">
</p>
<p>We&rsquo;re looking at a network with CIDR <code>192.165.0.0/24</code> behind a router. This can be an existing home router connected to the ISP, or another dedicated hardware router connected to the home gateway. The general idea here is that the network address range is split into two parts: DHCP addresses that are dynamically assigned to clients connecting to the network and the reserved static address range to be used for the physical nodes and VMs. Static addressing of the nodes is important for deployment automation that is using host addresses to connect to the hosts and apply changes. This would also allow other devices to connect to services running on Kubernetes using the local network addresses. While this network setup is pretty naive for a potentially internet-facing deployment, it should be considered as a basic building block of the infrastructure that can be implemented in a variety of ways (e.g. by using VLANs).</p>
<p>When dealing with virtualization, it is important to take into account the overhead it brings both when running a hypervisor and when deciding on the number of virtual machines to create on a physical node. <a href=https://www.proxmox.com/en/proxmox-ve>Proxmox VE</a> is an open-source small-footprint hypervisor that is based on Debian Linux and will be used for virtualization in this guide. One of the additional benefits it has is a Terraform provider that allows to declaratively define virtual machines based on templates and to provision them automatically.</p>
<h3 id=infrastructure-automation-with-proxmox-and-terraform>Infrastructure automation with Proxmox and Terraform</h3>
<p>When working with on-premises environments the infrastructure provisioning might be a tedious task. However, with an intermediate hypervisor layer, it is possible to achieve the same automation levels as with public cloud providers. <a href=https://registry.terraform.io/providers/Telmate/proxmox/latest/docs>Terraform Proxmox provider</a> brings Infrastructure-as-a-Code capabilities for environments running Proxmox.</p>
<blockquote>
<p><strong>NOTE:</strong> In order to continue, Proxmox VE must be installed on the target machines. To install Proxmox VE, follow the <a href=https://www.proxmox.com/en/proxmox-ve/get-started>official documentation</a>.</p>
</blockquote>
<p>Prior to the provisioning of the VMs themselves, it is beneficial to create a <code>cloud-init</code> template to simplify the configuration and provisioning of the future VMs. The template can be created manually on the PVE host as described in several blog posts such as <a href=https://norocketscience.at/deploy-proxmox-virtual-machines-using-cloud-init/>Deploy Proxmox virtual machines using cloud-init</a>, or we can use Ansible to automate this step. A working Ansible playbook with the instructions can be found at <a href=https://github.com/datastrophic/kubernetes-deployment/tree/master/proxmox/>datastrophic/kubernetes-deployment/proxmox/</a>.</p>
<p>Once the VM template is created, we can define a Terraform configuration to provision VMs. Here&rsquo;s an excerpt from <code>main.tf</code> with full instructions available at <a href=https://github.com/datastrophic/kubernetes-deployment/tree/master/proxmox/>datastrophic/kubernetes-deployment/proxmox/</a>:</p>
<pre tabindex=0><code>resource &quot;proxmox_vm_qemu&quot; &quot;control_plane&quot; {
  count             = 1
  name              = &quot;control-plane-${count.index}.k8s.cluster&quot;
  target_node       = &quot;${var.pm_node}&quot;

  clone             = &quot;ubuntu-2004-cloudinit-template&quot;

  os_type           = &quot;cloud-init&quot;
  cores             = 4
  sockets           = &quot;1&quot;
  cpu               = &quot;host&quot;
  memory            = 2048
  scsihw            = &quot;virtio-scsi-pci&quot;
  bootdisk          = &quot;scsi0&quot;

  disk {
    size            = &quot;20G&quot;
    type            = &quot;scsi&quot;
    storage         = &quot;local-lvm&quot;
    iothread        = 1
  }

  network {
    model           = &quot;virtio&quot;
    bridge          = &quot;vmbr0&quot;
  }

  # cloud-init settings
  # adjust the ip and gateway addresses as needed
  ipconfig0         = &quot;ip=192.168.0.11${count.index}/24,gw=192.168.0.1&quot;
  sshkeys = file(&quot;${var.ssh_key_file}&quot;)
}
</code></pre><p>A few things in the above configuration to pay attention to:</p>
<ul>
<li><code>clone</code> must point to the unique name of the VM template created at the previous step</li>
<li><code>ipconfig0</code> should respect the configuration of the network the VMs are running in. In this case, we assign VMs static IP addresses within the external (to PVE) network range so they look like regular hosts without the need for NAT routing.</li>
</ul>
<p>Once the configuration is adjusted for the target environment needs, it is sufficient to run terraform to get
target VMs created:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>terraform init
terraform plan -var<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;pm_user=&lt;PVE user&gt;&#34;</span> -var<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;pm_password=&lt;PVE password&gt;&#34;</span> -out plan

terraform apply <span style=color:#f1fa8c>&#34;plan&#34;</span>
</code></pre></div><h2 id=installing-the-kubernetes-cluster-and-essentials>Installing the Kubernetes cluster and essentials</h2>
<p>The <a href=https://landscape.cncf.io/>CNCF technology landscape</a> is broad and there&rsquo;s a lot of vendors providing solutions for various aspects of Kubernetes, and whole Kubernetes distributions themselves. A fully-functioning Kubernetes cluster requires several essential things such as container runtime, a Kubernetes distribution itself, a CNI (Container Network Interface) implementation for pod networking, a networking load balancer for exposing <code>Services</code> with <code>LoadBalancer</code> type on-premises, and a CSI (Container Storage Interface) implementation for volume provisioning.</p>
<p>Unlike <a href=https://github.com/kelseyhightower/kubernetes-the-hard-way>&ldquo;Kubernetes the Hard Way&rdquo;</a>, this guide relies on Ansible automation for the Kubernetes deployment and just covers the high-level steps required for the Kubernetes cluster bootstrap. Under the hood, the automation is using <code>kubeadm</code> in conjunction with declarative configuration for the cluster deployment. The source code of Ansible playbooks is available at <a href=https://github.com/datastrophic/kubernetes-deployment/>github.com/datastrophic/kubernetes-deployment</a>.</p>
<h4 id=before-you-begin>Before you begin</h4>
<p>Prior to going forward with the installation, it is recommended to clone the <a href=https://github.com/datastrophic/kubernetes-deployment/>source code repository for this guide</a> locally and double-check and update the following files to match your environment:</p>
<ul>
<li>the Ansible inventory file that contains the addresses of the nodes in <a href=https://github.com/datastrophic/kubernetes-deployment/blob/master/ansible/inventory.yaml>kubernetes-deployment/ansible/inventory.yaml</a></li>
<li>the default Ansible variables in <a href=https://github.com/datastrophic/kubernetes-deployment/blob/master/ansible/group_vars/all>kubernetes-deployment/ansible/group_vars/all</a> that contain Kubernetes version, MetalLB address range, etc.</li>
</ul>
<p>The client machine must have SSH access to the cluster nodes and <code>sudo</code> privileges on the target hosts.</p>
<h3 id=kubeadm-containerd-and-calico>kubeadm, containerd, and Calico</h3>
<p>In this guide, the Kubernetes distribution of choice is the vanilla open-source Kubernetes that comes with <a href=https://kubernetes.io/docs/reference/setup-tools/kubeadm/>kubeadm</a> tool for cluster bootstrapping. Vanilla Kubernetes has a bigger footprint compared to e.g. <a href=https://k3s.io/>k3s</a> and might not be a good fit for resource-constrained environments.
However, it is vendor independent and fully open-source, doesn&rsquo;t have any modifications, and both the API changes and
the tooling have the same release cadence so there&rsquo;s a lower risk of running into incompatibilities or delays.</p>
<p>Prior to deploying the Kubernetes itself, the cluster nodes require additional configuration and software installed:</p>
<ul>
<li>Nodes must have swap disabled, iptables enabled, and allow forwarding and bridged traffic as per <a href=https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/>Bootstrapping clusters with kubeadm</a>.</li>
<li>Nodes must have container runtime installed. The most standard container runtime used in various cloud and vendor Kubernetes distributions is containerd, so we will use it. Additional information on why we&rsquo;re not going to use Docker can be found in <a href=https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/>Don&rsquo;t Panic: Kubernetes and Docker</a>.</li>
<li>Nodes must have the following packages installed: <code>kubelet</code>, <code>kubectl</code>, and <code>kubeadm</code>. These can be installed via the standard package manager such as <code>apt</code>.</li>
</ul>
<p>There&rsquo;s a dedicated playbook for bootstrapping the nodes with all the required configuration and dependencies available at <a href=https://github.com/datastrophic/kubernetes-deployment/blob/master/ansible/bootstrap.yaml>ansible/bootstrap.yaml</a>. Double-check the <a href=#before-you-continue>defaults</a>, and from the root of the repo run:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>ansible-playbook -i ansible/inventory.yaml ansible/bootstrap.yaml -K
</code></pre></div><p>Once all the prerequisites are in place, we can use <code>kubeadm</code> for the cluster bootstrapping. The Kubernetes cluster installation consists of two major steps: bootstrapping of the control plane and joining the worker nodes. We can do it by running <a href=https://github.com/datastrophic/kubernetes-deployment/blob/master/ansible/kubernetes-install.yaml>ansible/kubernetes-install.yaml</a> playbook:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>ansible-playbook -i ansible/inventory.yaml ansible/kubernetes-install.yaml -K
</code></pre></div><p>The playbook runs <code>kubeadm init</code> on the control plane nodes and uses a declarative cluster configuration which is the <a href=https://pkg.go.dev/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta2>preferred way of configuring kubeadm</a>. The configuration template is available at <a href=https://github.com/datastrophic/kubernetes-deployment/blob/master/ansible/roles/kubeadm-init/templates/kubeadm.yaml>ansible/roles/kubeadm-init/templates/kubeadm.yaml</a>. Once the control plane bootstrap is complete, Ansible fetches a token and a certificate hash that are required for the worker nodes to authenticate with the API Server and runs <code>kubeadm join</code> on the worker nodes.</p>
<p>The playbook also deploys <a href=https://www.tigera.io/project-calico/>Calico</a> for cluster networking although <a href=https://kubernetes.io/docs/concepts/cluster-administration/networking/>multiple options are available</a>. The choice of Calico is motivated by it being the most widely adopted networking and security solution for Kubernetes (at the moment of writing).</p>
<p>Once the playbook execution completes, a kubeconfig file <code>admin.conf</code> will be fetched to the current directory. To verify the cluster is bootstrapped and connected, run:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>$&gt; kubectl --kubeconfig<span style=color:#ff79c6>=</span>admin.conf get nodes
NAME                          STATUS   ROLES                  AGE     VERSION
control-plane-0.k8s.cluster   Ready    control-plane,master   4m40s   v1.21.6
worker-0                      Ready    &lt;none&gt;                 4m5s    v1.21.6
worker-1                      Ready    &lt;none&gt;                 4m5s    v1.21.6
worker-2                      Ready    &lt;none&gt;                 4m4s    v1.21.6
</code></pre></div><blockquote>
<p>NOTE: it is recommended to export <code>admin.conf</code> location to run <code>kubectl</code> commands without providing <code>--kubeconfig</code> flag every time:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=color:#8be9fd;font-style:italic>export</span> <span style=color:#8be9fd;font-style:italic>KUBECONFIG</span><span style=color:#ff79c6>=</span><span style=color:#ff79c6>$(</span><span style=color:#8be9fd;font-style:italic>pwd</span><span style=color:#ff79c6>)</span>/admin.conf
</code></pre></div></blockquote>
<h3 id=essential-software>Essential software</h3>
<p>With the Kubernetes cluster up and running we now can deploy and run containers on it. However, a couple of essential parts of the fully-functional cluster are still missing: the dynamic volume provisioning and the support for <code>Services</code> with <code>LoadBalancer</code> type.</p>
<h4 id=volume-provisioning-with-openebs>Volume Provisioning with OpenEBS</h4>
<p>The volume provisioner solution comes in handy in both the situations when 3rd-party applications require a default <code>StorageClass</code> to provision <code>PersistentVolumes</code> and also when data replication is required for high availability guarantees.</p>
<p>Using <a href=https://openebs.io/>OpenEBS</a> for the home lab setup seems reasonable as it provides <a href=https://openebs.io/docs/concepts/casengines#local-engines>Local Engines</a> for provisioning <code>PersistentVolumes</code> backed directly by the local disks on hosts that should make the IO pretty fast. If data replication is required, OpenEBS has several <a href=https://openebs.io/docs/concepts/casengines#replicated-engines>Replicated Engines</a> but the performance of those varies.</p>
<p>Another alternative considered was <a href=https://rook.io/>Rook</a> that provides <a href=https://rook.io/docs/rook/v1.7/quickstart.html>multiple file access APIs</a> such as block, shared file system, and object store; and also has several options for the backend storage. The main user-facing advantage of Rook for home lab purposes was the out-of-the-box support for RWX (<code>ReadWriteMany</code>) volumes. However, OpenEBS with its local <code>PersistentVolumes</code> looked like a lighter and simpler alternative compared to the Ceph-backed Rook even with the lack of RWX.</p>
<p>To deploy a minimal installation with host-local <code>PersistentVolumes</code>, OpenEBS provides a &ldquo;lite&rdquo; version:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://openebs.github.io/charts/openebs-operator-lite.yaml
</code></pre></div><p>Once the Operator is installed, create a <code>StorageClass</code> and annotate it as <strong>default</strong>. This would allow using OpenEBS for volume provisioning without the need to specify the <code>StorageClass</code> for <code>PersistentVolumes</code> every time:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f - <span style=color:#f1fa8c>&lt;&lt;EOF
</span><span style=color:#f1fa8c>apiVersion: storage.k8s.io/v1
</span><span style=color:#f1fa8c>kind: StorageClass
</span><span style=color:#f1fa8c>metadata:
</span><span style=color:#f1fa8c>  name: openebs-hostpath
</span><span style=color:#f1fa8c>  annotations:
</span><span style=color:#f1fa8c>    storageclass.kubernetes.io/is-default-class: &#34;true&#34;
</span><span style=color:#f1fa8c>    openebs.io/cas-type: local
</span><span style=color:#f1fa8c>    cas.openebs.io/config: |
</span><span style=color:#f1fa8c>      - name: StorageType
</span><span style=color:#f1fa8c>        value: &#34;hostpath&#34;
</span><span style=color:#f1fa8c>      - name: BasePath
</span><span style=color:#f1fa8c>        value: &#34;/var/openebs/local/&#34;
</span><span style=color:#f1fa8c>provisioner: openebs.io/local
</span><span style=color:#f1fa8c>volumeBindingMode: WaitForFirstConsumer
</span><span style=color:#f1fa8c>reclaimPolicy: Delete
</span><span style=color:#f1fa8c>EOF</span>
</code></pre></div><p>To verify the installation, there are steps available in the official <a href=https://openebs.io/docs/user-guides/localpv-hostpath#install-verification>OpenEBS documentation</a> but there is also an end-to-end example available at the end of this guide.</p>
<h4 id=a-network-load-balancer-with-metallb>A Network Load Balancer with MetalLB</h4>
<p>One last missing piece of functionality in the provisioned cluster is the ability to expose <code>Services</code> of the <code>LoadBalancer</code> type to the local network. When running in the cloud, this functionality is provided by the Kubernetes integrations with cloud providers and corresponding network-facing load balancers are provisioned by using the infrastructure provider. When running on bare metal, there&rsquo;s no such integration available in Kubernetes out-of-the-box.</p>
<p><a href=https://metallb.universe.tf/>MetalLB</a> is the most widely used solution for network load balancing, however <a href=https://medium.com/thermokline/comparing-k8s-load-balancers-2f5c76ea8f31>other solutions started to appear</a>.</p>
<p>MetalLB installation is configured via a <code>ConfigMap</code> and can contain multiple address pools:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#ff79c6>apiVersion</span>: v1
<span style=color:#ff79c6>kind</span>: ConfigMap
<span style=color:#ff79c6>metadata</span>:
  <span style=color:#ff79c6>namespace</span>: metallb-system
  <span style=color:#ff79c6>name</span>: config
<span style=color:#ff79c6>data</span>:
  <span style=color:#ff79c6>config</span>: |<span style=color:#f1fa8c>
</span><span style=color:#f1fa8c>    address-pools:
</span><span style=color:#f1fa8c>    - name: default
</span><span style=color:#f1fa8c>      protocol: layer2
</span><span style=color:#f1fa8c>      addresses:
</span><span style=color:#f1fa8c>      - &#34;{{ lab.metallb_address_range }}&#34;</span>    
</code></pre></div><p>The template above is a part of the Ansible <a href=https://github.com/datastrophic/kubernetes-deployment/blob/master/ansible/roles/metallb/templates/metallb-config.yaml>ansible/metallb.yaml</a> playbook that installs the MetalLB and configures it to allocate addresses from the <code>lab.metallb_address_range</code> variable specified in the <code>group_vars</code>. The address range must be relevant for the target environment (part of the reserved static address range described in the <a href=#deployment-layout>deployment layout section</a> so that the addresses can be allocated. To install MetalLB, run:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>ansible-playbook -i ansible/inventory.yaml ansible/metallb.yaml -K
</code></pre></div><h2 id=verifying-the-installation>Verifying the installation</h2>
<p>To verify the installation, we are going to create a <a href=https://min.io/>MinIO</a> <code>Deployment</code> with a <code>PersistentVolume</code> for storage, and expose the deployment to the local network via the <code>LoadBalancer</code> <code>Service</code> type. The example is based on the <a href=https://github.com/kubernetes/examples/tree/master/staging/storage/minio>Kubernetes storage examples</a>.</p>
<ol>
<li>Create a <code>PersistentVolumeClaim</code>:
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f - <span style=color:#f1fa8c>&lt;&lt;EOF
</span><span style=color:#f1fa8c>apiVersion: v1
</span><span style=color:#f1fa8c>kind: PersistentVolumeClaim
</span><span style=color:#f1fa8c>metadata:
</span><span style=color:#f1fa8c>  name: minio-pv-claim
</span><span style=color:#f1fa8c>spec:
</span><span style=color:#f1fa8c>  accessModes:
</span><span style=color:#f1fa8c>    - ReadWriteOnce
</span><span style=color:#f1fa8c>  resources:
</span><span style=color:#f1fa8c>    requests:
</span><span style=color:#f1fa8c>      storage: 1Gi
</span><span style=color:#f1fa8c>EOF</span>
</code></pre></div></li>
<li>Create a <code>Deployment</code>:
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f - <span style=color:#f1fa8c>&lt;&lt;EOF
</span><span style=color:#f1fa8c>apiVersion: apps/v1
</span><span style=color:#f1fa8c>kind: Deployment
</span><span style=color:#f1fa8c>metadata:
</span><span style=color:#f1fa8c>  name: minio-deployment
</span><span style=color:#f1fa8c>spec:
</span><span style=color:#f1fa8c>  selector:
</span><span style=color:#f1fa8c>    matchLabels:
</span><span style=color:#f1fa8c>      app: minio
</span><span style=color:#f1fa8c>  strategy:
</span><span style=color:#f1fa8c>    type: Recreate
</span><span style=color:#f1fa8c>  template:
</span><span style=color:#f1fa8c>    metadata:
</span><span style=color:#f1fa8c>      labels:
</span><span style=color:#f1fa8c>        app: minio
</span><span style=color:#f1fa8c>    spec:
</span><span style=color:#f1fa8c>      volumes:
</span><span style=color:#f1fa8c>      - name: storage
</span><span style=color:#f1fa8c>        persistentVolumeClaim:
</span><span style=color:#f1fa8c>          claimName: minio-pv-claim
</span><span style=color:#f1fa8c>      containers:
</span><span style=color:#f1fa8c>      - name: minio
</span><span style=color:#f1fa8c>        image: minio/minio:latest
</span><span style=color:#f1fa8c>        args:
</span><span style=color:#f1fa8c>        - server
</span><span style=color:#f1fa8c>        - /storage
</span><span style=color:#f1fa8c>        - --console-address
</span><span style=color:#f1fa8c>        - &#34;:9001&#34;
</span><span style=color:#f1fa8c>        env:
</span><span style=color:#f1fa8c>        - name: MINIO_ACCESS_KEY
</span><span style=color:#f1fa8c>          value: &#34;minio&#34;
</span><span style=color:#f1fa8c>        - name: MINIO_SECRET_KEY
</span><span style=color:#f1fa8c>          value: &#34;minio123&#34;
</span><span style=color:#f1fa8c>        ports:
</span><span style=color:#f1fa8c>        - containerPort: 9000
</span><span style=color:#f1fa8c>          hostPort: 9000
</span><span style=color:#f1fa8c>        - containerPort: 9001
</span><span style=color:#f1fa8c>          hostPort: 9001
</span><span style=color:#f1fa8c>        volumeMounts:
</span><span style=color:#f1fa8c>        - name: storage
</span><span style=color:#f1fa8c>          mountPath: &#34;/storage&#34;
</span><span style=color:#f1fa8c>EOF</span>
</code></pre></div></li>
<li>Verify the <code>PersistentVolumeClaim</code> is bound and a <code>PersistentVolume</code> is created:
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get pvc

NAME             STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS       AGE
minio-pv-claim   Bound    pvc-f43856ab-d0a2-42d3-8088-3010f7966ab9   1Gi        RWO            openebs-hostpath   77s


kubectl get pv

NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                  STORAGECLASS       REASON   AGE
pvc-f43856ab-d0a2-42d3-8088-3010f7966ab9   1Gi        RWO            Delete           Bound    minio/minio-pv-claim   openebs-hostpath            2m42s
</code></pre></div></li>
<li>Verify the <code>Deployment</code> is healthy:
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl describe deployment minio-deployment

...
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   minio-deployment-877b8596f <span style=color:#ff79c6>(</span>1/1 replicas created<span style=color:#ff79c6>)</span>
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  7m4s  deployment-controller  Scaled up replica <span style=color:#8be9fd;font-style:italic>set</span> minio-deployment-877b8596f to <span style=color:#bd93f9>1</span>
</code></pre></div></li>
<li>Expose the <code>Deployment</code> via a <code>Service</code> of the <code>LoadBalancer</code> type:
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f - <span style=color:#f1fa8c>&lt;&lt;EOF
</span><span style=color:#f1fa8c>apiVersion: v1
</span><span style=color:#f1fa8c>kind: Service
</span><span style=color:#f1fa8c>metadata:
</span><span style=color:#f1fa8c>  name: minio
</span><span style=color:#f1fa8c>spec:
</span><span style=color:#f1fa8c>  ports:
</span><span style=color:#f1fa8c>  - name: http
</span><span style=color:#f1fa8c>    port: 9000
</span><span style=color:#f1fa8c>    protocol: TCP
</span><span style=color:#f1fa8c>    targetPort: 9000
</span><span style=color:#f1fa8c>  - name: http-ui
</span><span style=color:#f1fa8c>    port: 9001
</span><span style=color:#f1fa8c>    protocol: TCP
</span><span style=color:#f1fa8c>    targetPort: 9001
</span><span style=color:#f1fa8c>  selector:
</span><span style=color:#f1fa8c>    app: minio
</span><span style=color:#f1fa8c>  type: LoadBalancer
</span><span style=color:#f1fa8c>EOF</span>
</code></pre></div></li>
<li>Verify the <code>Service</code> is created and has the External IP set. For example:
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get service minio
NAME    TYPE           CLUSTER-IP       EXTERNAL-IP      PORT<span style=color:#ff79c6>(</span>S<span style=color:#ff79c6>)</span>          AGE
minio   LoadBalancer   10.109.223.141   192.168.0.151   9000:31073/TCP   7s
</code></pre></div></li>
</ol>
<p>The <code>EXTERNAL-IP</code> address should be from the local network range, and now, you should be able to navigate to <a href=http://EXTERNAL-IP:9001>http://EXTERNAL-IP:9001</a> from a browser and see the MinIO Console login screen.</p>
<p>
<img src=/blog/2021-12-01/minio-login-screen.png alt="MinIO Login Screen">
</p>
<p>The default credentials are specified in the MinIO <code>Deployment</code> are <code>minio</code> and <code>minio123</code> for login and password correspondingly. After the login, create a bucket named <code>test</code>, and let&rsquo;s verify it is created on the <code>PersistentVolume</code>:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl <span style=color:#8be9fd;font-style:italic>exec</span> deploy/minio-deployment -- bash -c <span style=color:#f1fa8c>&#34;ls -la /storage&#34;</span>

total <span style=color:#bd93f9>16</span>
drwxrwxrwx <span style=color:#bd93f9>4</span> root root <span style=color:#bd93f9>4096</span> Dec  <span style=color:#bd93f9>1</span> 19:04 .
drwxr-xr-x <span style=color:#bd93f9>1</span> root root <span style=color:#bd93f9>4096</span> Dec  <span style=color:#bd93f9>1</span> 19:00 ..
drwxr-xr-x <span style=color:#bd93f9>6</span> root root <span style=color:#bd93f9>4096</span> Dec  <span style=color:#bd93f9>1</span> 18:39 .minio.sys
drwxr-xr-x <span style=color:#bd93f9>2</span> root root <span style=color:#bd93f9>4096</span> Dec  <span style=color:#bd93f9>1</span> 19:04 <span style=color:#8be9fd;font-style:italic>test</span>
</code></pre></div><p>That wraps the verification: <code>test</code> folder created from the UI exposed to the local network was saved on the <code>PersistentVolume</code> mounted at <code>/storage</code> path.</p>
<h2 id=observability>Observability</h2>
<p>The final important piece of any permanent cluster is the observability stack. Depending on your cluster size,
it could be just an instance of the <a href=https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/>Kubernetes Dashboard</a> or the <a href=https://prometheus-operator.dev/>Prometheus Operator</a>. This guide focuses
on the Kubernetes Dashboard but it is important to note that it doesn&rsquo;t provide any historical data view,
custom dashboarding, or alerting. If those features are must have for your cluster - the <a href=https://prometheus-operator.dev/>Prometheus Operator</a> would be a great place to start.</p>
<h3 id=kubernetes-dashboard>Kubernetes Dashboard</h3>
<p>If the cluster is constrained in resources so it is hard to squeeze the full Prometheus stack
onto it, then the Kubernetes Dashboard would be the must-have minimum solution for the observability.
The Kubernetes Dashboard has its <a href=https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/>respective installation guide</a> and here we&rsquo;ll focus on the appropriate RBAC permissions
for the <code>ServiceAccount</code> used by it.</p>
<p>First, let&rsquo;s install the Kubernetes Dashboard:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.4.0/aio/deploy/recommended.yaml
</code></pre></div><p>While the Kubernetes Dashboard allows creating new resources and editing the existing ones,
using it in read-only mode is more secure and wouldn&rsquo;t impose any security risks should
anybody gain the access to the UI. The scope of visibility of the Dashboard is
controlled via RBAC of the users accessing it.</p>
<p>The most conservative approach would be to use an <a href=https://kubernetes.io/docs/reference/access-authn-authz/rbac/#aggregated-clusterroles>Aggregated ClusterRole</a> based on the default <a href=https://kubernetes.io/docs/reference/access-authn-authz/rbac/#user-facing-roles>viewer role</a> and extend it with additional rules as needed:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f - <span style=color:#f1fa8c>&lt;&lt;EOF
</span><span style=color:#f1fa8c>apiVersion: rbac.authorization.k8s.io/v1
</span><span style=color:#f1fa8c>kind: ClusterRole
</span><span style=color:#f1fa8c>metadata:
</span><span style=color:#f1fa8c>  name: dashboard-viewer
</span><span style=color:#f1fa8c>aggregationRule:
</span><span style=color:#f1fa8c>  clusterRoleSelectors:
</span><span style=color:#f1fa8c>  - matchLabels:
</span><span style=color:#f1fa8c>      rbac.authorization.k8s.io/aggregate-to-view: &#34;true&#34;
</span><span style=color:#f1fa8c>  - matchLabels:
</span><span style=color:#f1fa8c>      rbac.homelab.k8s.io/aggregate-to-view: &#34;true&#34;
</span><span style=color:#f1fa8c>rules: []
</span><span style=color:#f1fa8c>
</span><span style=color:#f1fa8c>---
</span><span style=color:#f1fa8c>apiVersion: rbac.authorization.k8s.io/v1
</span><span style=color:#f1fa8c>kind: ClusterRole
</span><span style=color:#f1fa8c>metadata:
</span><span style=color:#f1fa8c>  name: dashboard-extended-view
</span><span style=color:#f1fa8c>  labels:
</span><span style=color:#f1fa8c>    rbac.homelab.k8s.io/aggregate-to-view: &#34;true&#34;
</span><span style=color:#f1fa8c>rules:
</span><span style=color:#f1fa8c>- apiGroups:
</span><span style=color:#f1fa8c>  - &#34;&#34;
</span><span style=color:#f1fa8c>  resources:
</span><span style=color:#f1fa8c>  - nodes
</span><span style=color:#f1fa8c>  - extensions
</span><span style=color:#f1fa8c>  - apps
</span><span style=color:#f1fa8c>  - batch
</span><span style=color:#f1fa8c>  - storage
</span><span style=color:#f1fa8c>  - networking
</span><span style=color:#f1fa8c>  verbs:
</span><span style=color:#f1fa8c>  - get
</span><span style=color:#f1fa8c>  - list
</span><span style=color:#f1fa8c>  - watch
</span><span style=color:#f1fa8c>EOF</span>
</code></pre></div><p>The <code>ClusterRole</code> provides extended view permissions but still doesn&rsquo;t allow viewing
<code>Secrets</code> and resources from <code>rbac.authorization.k8s.io</code> API group. Now, let&rsquo;s create
a dedicated <code>ServiceAccount</code> and bind it to the created <code>ClusterRole</code>:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f - <span style=color:#f1fa8c>&lt;&lt;EOF
</span><span style=color:#f1fa8c>apiVersion: v1
</span><span style=color:#f1fa8c>kind: ServiceAccount
</span><span style=color:#f1fa8c>metadata:
</span><span style=color:#f1fa8c>  name: dashboard-viewer
</span><span style=color:#f1fa8c>  namespace: kubernetes-dashboard
</span><span style=color:#f1fa8c>
</span><span style=color:#f1fa8c>---
</span><span style=color:#f1fa8c>apiVersion: rbac.authorization.k8s.io/v1
</span><span style=color:#f1fa8c>kind: ClusterRoleBinding
</span><span style=color:#f1fa8c>metadata:
</span><span style=color:#f1fa8c>  name: dashboard-viewer
</span><span style=color:#f1fa8c>roleRef:
</span><span style=color:#f1fa8c>  apiGroup: rbac.authorization.k8s.io
</span><span style=color:#f1fa8c>  kind: ClusterRole
</span><span style=color:#f1fa8c>  name: dashboard-viewer
</span><span style=color:#f1fa8c>subjects:
</span><span style=color:#f1fa8c>- kind: ServiceAccount
</span><span style=color:#f1fa8c>  name: dashboard-viewer
</span><span style=color:#f1fa8c>  namespace: kubernetes-dashboard
</span><span style=color:#f1fa8c>EOF</span>
</code></pre></div><p>The Dashboard can be accessed either via <code>kubectl proxy</code>, or via port forwarding:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl -n kubernetes-dashboard port-forward service/kubernetes-dashboard 8443:443
</code></pre></div><p>The Dashboard will be available at <a href=https://localhost:8443/>https://localhost:8443/</a>.</p>
<p>
<img src=/blog/2021-12-01/k8s-dashboard-login.png alt="Kubernetes Dashboard Login">
</p>
<p>To discover the <code>ServiceAccount</code> token for accessing the Dashboard, run:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl -n kubernetes-dashboard get secret <span style=color:#ff79c6>$(</span>kubectl -n kubernetes-dashboard get sa/dashboard-viewer -o <span style=color:#8be9fd;font-style:italic>jsonpath</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;{.secrets[0].name}&#34;</span><span style=color:#ff79c6>)</span> -o go-template<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;{{.data.token | base64decode}}&#34;</span>
</code></pre></div><p>
<img src=/blog/2021-12-01/k8s-dashboard.png alt="Kubernetes Dashboard">
</p>
<p>The Dashboard will display notifications about the inability to list <code>Secrets</code> or resources
from the <code>rbac.authorization.k8s.io</code> API group. This is expected because the <code>ClusterRole</code> doesn&rsquo;t
allow that.</p>
<h2 id=conclusion>Conclusion</h2>
<p>There&rsquo;s been a lot described in this guide and that might be overwhelming.
Although we have a fully functioning Kubernetes cluster suitable for a local network,
it&rsquo;s not the end of the story yet. If it is planned for the cluster to be multi-tenant -
then it will require an integrated solution for AuthN/Z such as Dex. Also, this guide doesn&rsquo;t
cover how to set up and configure TLS-secured Ingress and authenticated access for the
services deployed on the cluster. Both of these topics will be covered in later posts.</p>
<nav class=pagination role=navigation>
<a class=older-posts href=/secure-kubeflow-ingress-and-authentication/>Next Post &rarr;</a>
<a class=newer-posts href=/kubeflow-training-operators-and-istio-solving-the-proxy-sidecar-lifecycle-problem-for-aiml-workloads/>&larr; Previous Post</a>
</nav>
</div>
<div class="col-lg-2 col-lg-offset-0
visible-lg-block
sidebar-container
catalog-container">
<div class=side-catalog>
<hr class="hidden-sm hidden-xs">
<h5>
<a class=catalog-toggle href=#>CONTENTS</a>
</h5>
<ul class=catalog-body></ul>
</div>
</div>
<div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
sidebar-container">
<section>
<hr class="hidden-sm hidden-xs">
<h5><a href=/tags/>FEATURED TAGS</a></h5>
<div class=tags>
<a href=/tags/akka title=akka>
akka
</a>
<a href=/tags/cassandra title=cassandra>
cassandra
</a>
<a href=/tags/istio title=istio>
istio
</a>
<a href=/tags/kubeflow title=kubeflow>
kubeflow
</a>
<a href=/tags/kubernetes title=kubernetes>
kubernetes
</a>
<a href=/tags/mesos title=mesos>
mesos
</a>
<a href=/tags/sheduling title=sheduling>
sheduling
</a>
<a href=/tags/spark title=spark>
spark
</a>
</div>
</section>
</div>
</div>
</div>
</article>
<footer class="site-footer clearfix">
<section class=copyright><a href>datastrophic</a> &copy; 2021</section>
<section class=poweredby>Proudly generated by <a class=icon-hugo href=http://gohugo.io>HUGO</a></section>
</footer>
<script>function loadAsync(f,b){var c=document,d='script',a=c.createElement(d),e=c.getElementsByTagName(d)[0];a.src=f,b&&a.addEventListener('load',function(a){b(null,a)},!1),e.parentNode.insertBefore(a,e)}</script>
<script>$('#tag_cloud').length!==0&&loadAsync("/js/jquery.tagcloud.js",function(){$.fn.tagcloud.defaults={color:{start:'#bbbbee',end:'#0085a1'}},$('#tag_cloud a').tagcloud()})</script>
<script>loadAsync("https://cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.js",function(){var a=document.querySelector("nav");a&&FastClick.attach(a)})</script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga('create','UA-65032691-1','auto'),ga('send','pageview'))</script>
<script async src=https://www.google-analytics.com/analytics.js></script>
<script type=text/javascript>function generateCatalog(a){_containerSelector='div.post-container';var h=$(_containerSelector),c,d,e,f,g,b;return c=h.find('h1,h2,h3,h4,h5,h6'),$(a).html(''),c.each(function(){d=$(this).prop('tagName').toLowerCase(),g="#"+$(this).prop('id'),e=$(this).text(),b=$('<a href="'+g+'" rel="nofollow">'+e+'</a>'),f=$('<li class="'+d+'_nav"></li>').append(b),$(a).append(f)}),!0}generateCatalog(".catalog-body"),$(".catalog-toggle").click(function(a){a.preventDefault(),$('.side-catalog').toggleClass("fold")}),loadAsync("/js/jquery.nav.js",function(){$('.catalog-body').onePageNav({currentClass:"active",changeHash:!1,easing:"swing",filter:"",scrollSpeed:700,scrollOffset:0,scrollThreshold:.2,begin:null,end:null,scrollChange:null,padding:80})})</script>
</body>
</html>